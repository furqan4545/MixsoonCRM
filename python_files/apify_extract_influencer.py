
import logging
import pandas as pd
import os
import sys
from datetime import datetime
from apify_client import ApifyClient
import time
import re
import numpy as np

# =========================================================
# 0. [í•µì‹¬] ë¦¬ì†ŒìŠ¤ ê²½ë¡œ ë° ì„¤ì •
# =========================================================

# 1. Apify API í‚¤ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
APIFY_API_KEY = os.environ.get("APIFY_API_KEY", "")

# 2. CSV ì…ë ¥ ë° ì¶œë ¥ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CSV_INPUT_PATH = os.path.join(BASE_DIR, "csv_data", "Result_Worker_1_20260128_1133.csv")
USERNAME_COLUMN = "Username"
USERNAME_LIMIT = 50  # -1ì´ë©´ ì „ì²´, 50ì´ë©´ 50ëª…ë§Œ ìˆ˜ì§‘

OUTPUT_DIR = os.path.join(BASE_DIR, "output")
OUTPUT_FILENAME = "output.xlsx"

# 3. ìˆ˜ì§‘ ì˜µì…˜
TARGET_VIDEO_COUNT = 20

# =========================================================
# 2. ë©”ì¸ ì‹¤í–‰ ë¡œì§
# =========================================================

print("â³ ì„¤ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

# Apify ì„¤ì • (ì½”ë“œ ë‚´ì¥)
client = ApifyClient(APIFY_API_KEY)
print(f"ğŸ”‘ API Key ì„¤ì • ì™„ë£Œ")

try:
    df = pd.read_csv(CSV_INPUT_PATH, encoding="utf-8-sig")
except Exception as e:
    print(f"âŒ CSV ì½ê¸° ì‹¤íŒ¨:\n{e}")
    sys.exit(1)

if USERNAME_COLUMN not in df.columns:
    print(f"âŒ CSVì— '{USERNAME_COLUMN}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit(1)

df1 = df

# ---------------------------------------------------------
# APIFY ìˆ˜ì§‘ ë¡œì§
# ---------------------------------------------------------

raw_profiles = df1[USERNAME_COLUMN].dropna().astype(str).tolist()

all_profiles = list(dict.fromkeys(raw_profiles))

if USERNAME_LIMIT != -1:
    all_profiles = all_profiles[:USERNAME_LIMIT]

print(f"â„¹ï¸ ì…ë ¥: {len(raw_profiles)}ëª… -> ìˆ˜ì§‘ ëŒ€ìƒ: {len(all_profiles)}ëª…")

all_results = []
BATCH_SIZE = 100

print(f" ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ì´ {len(all_profiles)}ëª…)")

for i in range(0, len(all_profiles), BATCH_SIZE):
    batch = all_profiles[i : i + BATCH_SIZE]
    current_batch_num = i // BATCH_SIZE + 1
    
    total_max_items = len(batch) * TARGET_VIDEO_COUNT 

    batch_start_time = time.time()
    print(f"\nğŸš€ [{current_batch_num}íšŒì°¨] {len(batch)}ëª… ( {batch[0]} ... ) ë¶„ì„ ì‹œì‘")

    run_input = {
        "maxItems": total_max_items + 300,
        "usernames": batch,
        "resultsPerPage": TARGET_VIDEO_COUNT
    }

    actor_run = client.actor("ssOXktOBaQQiYfhc4").start(run_input=run_input)
    run_id = actor_run["id"]
    
    print(f"    â³ ë°ì´í„° ìˆ˜ì§‘ ì§„í–‰ ì¤‘...")
    run_finished = client.run(run_id).wait_for_finish()
    
    dataset_id = run_finished["defaultDatasetId"]
    current_items = list(client.dataset(dataset_id).iterate_items())
    
    all_results.extend(current_items)

    batch_end_time = time.time()
    elapsed_time = batch_end_time - batch_start_time
    print(f"âœ… [{current_batch_num}íšŒì°¨] ì™„ë£Œ! {len(current_items)}ê°œ ë°ì´í„° í™•ë³´.")

print(f"ğŸ‰ ìˆ˜ì§‘ ì¢…ë£Œ. ì´ {len(all_results)}ê°œ ë°ì´í„°.")

# 1. ì›ë³¸ ëª…ë‹¨ ì¤€ë¹„ (ê¸°ì¤€ì )
original_df = pd.DataFrame({'name': all_profiles})
original_df['merge_key'] = original_df['name'].astype(str).str.lower().str.strip()

if all_results:
    final_df = pd.DataFrame(all_results)
    
    def safe_extract(df, col, key):
        if col in df.columns:
            return df[col].apply(lambda x: x.get(key) if isinstance(x, dict) else None)
        return None

    # -- [1] í”„ë¡œí•„ ë°ì´í„° ì¶”ì¶œ --
    final_df['name_extracted'] = safe_extract(final_df, 'channel', 'username')
    final_df['name_origin'] = final_df['name_extracted'].fillna("Unknown")
    
    final_df['merge_key'] = final_df['name_origin'].astype(str).str.lower().str.strip()
    
    if 'uploadedAtFormatted' in final_df.columns:
        final_df = final_df.sort_values(by=['merge_key', 'uploadedAtFormatted'], ascending=[True, False])
    
    final_df = final_df.groupby('merge_key').head(TARGET_VIDEO_COUNT)

    # -- [2] í”„ë¡œí•„ ì •ë³´ ì§‘ê³„ --
    final_df['profile_url'] = safe_extract(final_df, 'channel', 'url')
    final_df['biolink']     = safe_extract(final_df, 'channel', 'bio')
    final_df['fans']        = safe_extract(final_df, 'channel', 'followers')

    profile_agg = final_df.groupby('merge_key', as_index=False).agg(
        profile_url=('profile_url', 'first'),
        biolink=('biolink', 'first'),
        fans=('fans', 'max')
    )

    # -- [3] ì˜ìƒ í†µê³„ ì§‘ê³„ --
    if 'views' in final_df.columns and 'bookmarks' in final_df.columns:
        video_agg = final_df.groupby('merge_key', as_index=False).agg(
            play_median=('views', 'median'),
            collect_median=('bookmarks', 'median'),
            signature=('title', 'first'), 
            last_post_date=('uploadedAtFormatted', 'max')
        )
    else:
        video_agg = pd.DataFrame(columns=['merge_key', 'play_median', 'collect_median', 'signature', 'last_post_date'])

    # -- [4] ì´ë¯¸ì§€ ì¶”ì¶œ --
    if 'video' in final_df.columns:
        final_df['thumbnail'] = safe_extract(final_df, 'video', 'cover')
        image_df = final_df.groupby('merge_key')['thumbnail'].apply(lambda x: pd.Series(x.values)).unstack()
        image_df.columns = [f'image_{i+1}' for i in image_df.columns]
        image_df = image_df.reset_index()
    else:
        image_df = pd.DataFrame(columns=['merge_key'])

    # -- [5] ë³‘í•© --
    step1 = pd.merge(profile_agg, video_agg, on='merge_key', how='left')
    scraped_result = pd.merge(step1, image_df, on='merge_key', how='left')

    def extract_email_from_text(text):
        if not isinstance(text, str): return ""
        match = re.search(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', text)
        return match.group(0) if match else ""

    print("âš¡ ì´ë©”ì¼ ì¶”ì¶œ ì¤‘...")
    scraped_result['email'] = scraped_result['biolink'].apply(extract_email_from_text)

else:
    print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ê°’ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.")
    scraped_result = pd.DataFrame(columns=['merge_key'])

# 3. [ìµœì¢… ë³‘í•©]
final_complete_df = pd.merge(original_df, scraped_result, on='merge_key', how='left')
final_complete_df = final_complete_df.drop(columns=['merge_key'])
final_complete_df = final_complete_df.fillna("NA")

# 4. ì—´ ìˆœì„œ ë° ì—…ë¡œë“œ
fixed_cols = ['name', 'profile_url', 'signature', 'biolink', 'fans', 'play_median','collect_median', 'last_post_date', 'email']
image_cols = [f'image_{i}' for i in range(1, TARGET_VIDEO_COUNT + 1)]
target_order = fixed_cols + image_cols

final_cols = [c for c in target_order if c in final_complete_df.columns]
final_complete_df = final_complete_df[final_cols]

os.makedirs(OUTPUT_DIR, exist_ok=True)
output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)
try:
    final_complete_df.to_excel(output_path, index=False)
    print(f"ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {output_path}")
except Exception as e:
    print(f"âŒ ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨: {e}")

